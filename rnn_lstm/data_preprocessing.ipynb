{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3f72ff",
   "metadata": {},
   "source": [
    "# Insightly - The Recurrent Neural Network Implementation\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998089a3",
   "metadata": {},
   "source": [
    "### Author: Ronald Li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439987b0",
   "metadata": {},
   "source": [
    "### Setup - import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0153b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee165201",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4d000",
   "metadata": {},
   "source": [
    "Load and inspect the e-commerce reviews dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c6c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_data.csv with shape: (4000, 8)\n",
      "Columns: ['name', 'brand', 'categories', 'primaryCategories', 'reviews.date', 'reviews.text', 'reviews.title', 'sentiment']\n",
      "After dropping NAs: 4000 rows\n",
      "Label distribution (all three sentiments):\n",
      "sentiment\n",
      "Positive    3749\n",
      "Neutral      158\n",
      "Negative      93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few cleaned rows (text + label):\n",
      "                                        reviews.text sentiment  label\n",
      "0  Purchased on Black FridayPros - Great Price (e...  Positive      2\n",
      "1  I purchased two Amazon in Echo Plus and two do...  Positive      2\n",
      "2  Just an average Alexa option. Does show a few ...   Neutral      1\n",
      "3  very good product. Exactly what I wanted, and ...  Positive      2\n",
      "4  This is the 3rd one I've purchased. I've bough...  Positive      2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = \"ecommerce_dataset\"\n",
    "train_path = os.path.join(base_dir, \"train_data.csv\")\n",
    "\n",
    "raw_df = pd.read_csv(train_path)\n",
    "print(f\"Loaded train_data.csv with shape: {raw_df.shape}\")\n",
    "print(\"Columns:\", raw_df.columns.tolist())\n",
    "\n",
    "# Keep only the text and sentiment columns, drop missing values\n",
    "text_col = \"reviews.text\"\n",
    "label_col = \"sentiment\"\n",
    "\n",
    "df = raw_df[[text_col, label_col]].dropna()\n",
    "print(f\"After dropping NAs: {df.shape[0]} rows\")\n",
    "\n",
    "# Keep all three sentiments: Negative, Neutral, Positive\n",
    "valid_labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "df = df[df[label_col].isin(valid_labels)].copy()\n",
    "print(\"Label distribution (all three sentiments):\")\n",
    "print(df[label_col].value_counts())\n",
    "\n",
    "# Map sentiment strings to 3-class labels: Negative -> 0, Neutral -> 1, Positive -> 2\n",
    "label2idx = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "df[\"label\"] = df[label_col].map(label2idx)\n",
    "\n",
    "print(\"\\nFirst few cleaned rows (text + label):\")\n",
    "print(df[[text_col, label_col, \"label\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4acaddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews after cleaning: 4000\n",
      "Example cleaned review:\n",
      "                                        reviews.text  \\\n",
      "0  Purchased on Black FridayPros - Great Price (e...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  purchased on black fridaypros great price even...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [purchased, on, black, fridaypros, great, pric...  \n"
     ]
    }
   ],
   "source": [
    "# Basic text cleaning and tokenization\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, remove non-letter characters, normalize whitespace.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"Simple whitespace tokenization.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# Apply cleaning and tokenization\n",
    "df[\"cleaned_text\"] = df[text_col].apply(clean_text)\n",
    "df = df[df[\"cleaned_text\"].str.len() > 0].copy()\n",
    "\n",
    "df[\"tokens\"] = df[\"cleaned_text\"].apply(tokenize)\n",
    "\n",
    "print(f\"Number of reviews after cleaning: {len(df)}\")\n",
    "print(\"Example cleaned review:\")\n",
    "print(df[[text_col, \"cleaned_text\", \"tokens\"]].head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e1a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 123,292\n",
      "Unique tokens: 4,778\n",
      "Vocabulary size (including <PAD>/<UNK>): 2944\n",
      "Most common tokens:\n",
      "  the: 5365\n",
      "  it: 4201\n",
      "  and: 3990\n",
      "  to: 3990\n",
      "  i: 3904\n",
      "  for: 2965\n",
      "  a: 2734\n",
      "  is: 2418\n",
      "  my: 2274\n",
      "  this: 2103\n"
     ]
    }
   ],
   "source": [
    "# Build a simple vocabulary based on token frequency\n",
    "\n",
    "all_tokens = []\n",
    "for toks in df[\"tokens\"]:\n",
    "    all_tokens.extend(toks)\n",
    "\n",
    "word_counts = Counter(all_tokens)\n",
    "print(f\"Total tokens: {len(all_tokens):,}\")\n",
    "print(f\"Unique tokens: {len(word_counts):,}\")\n",
    "\n",
    "min_freq = 2  # ignore very rare words\n",
    "\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word, count in word_counts.items():\n",
    "    if count >= min_freq:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size (including <PAD>/<UNK>): {vocab_size}\")\n",
    "\n",
    "# Quick peek at most frequent tokens\n",
    "print(\"Most common tokens:\")\n",
    "for token, count in Counter(word_counts).most_common(10):\n",
    "    print(f\"  {token}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcab0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length stats:\n",
      "  min: 1\n",
      "  max: 1606\n",
      "  mean: 30.823\n",
      "  95th percentile: 80.0\n",
      "\n",
      "Using fixed sequence length: 80\n",
      "Example padded sequence:\n",
      "                                        cleaned_text  \\\n",
      "0  purchased on black fridaypros great price even...   \n",
      "\n",
      "                                              padded  \n",
      "0  [2, 3, 4, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to integer sequences and pad to a fixed length\n",
    "\n",
    "def tokens_to_ids(tokens):\n",
    "    return [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "\n",
    "df[\"sequence\"] = df[\"tokens\"].apply(tokens_to_ids)\n",
    "\n",
    "seq_lengths = df[\"sequence\"].apply(len)\n",
    "print(\"Sequence length stats:\")\n",
    "print(\"  min:\", int(seq_lengths.min()))\n",
    "print(\"  max:\", int(seq_lengths.max()))\n",
    "print(\"  mean:\", float(seq_lengths.mean()))\n",
    "print(\"  95th percentile:\", float(np.percentile(seq_lengths, 95)))\n",
    "\n",
    "# Use the 95th percentile as a simple fixed length\n",
    "sequence_length = int(np.percentile(seq_lengths, 95))\n",
    "print(f\"\\nUsing fixed sequence length: {sequence_length}\")\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    if len(seq) >= max_len:\n",
    "        return seq[:max_len]\n",
    "    return seq + [pad_value] * (max_len - len(seq))\n",
    "\n",
    "\n",
    "df[\"padded\"] = df[\"sequence\"].apply(lambda s: pad_sequence(s, sequence_length))\n",
    "\n",
    "print(\"Example padded sequence:\")\n",
    "print(df[[\"cleaned_text\", \"padded\"]].head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c69cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (4000, 80)\n",
      "Label vector shape: (4000,)\n",
      "Label distribution (0=Negative, 1=Neutral, 2=Positive):\n",
      "{np.int64(0): np.int64(93), np.int64(1): np.int64(158), np.int64(2): np.int64(3749)}\n",
      "\n",
      "Train shape: (3200, 80) (3200,)\n",
      "Test shape: (800, 80) (800,)\n",
      "\n",
      "Saved:\n",
      "   ecommerce_dataset/X_train.npy\n",
      "   ecommerce_dataset/X_test.npy\n",
      "   ecommerce_dataset/y_train.npy\n",
      "   ecommerce_dataset/y_test.npy\n",
      "   ecommerce_dataset/vocab.pkl\n",
      "   ecommerce_dataset/label_mapping.pkl\n",
      "   ecommerce_dataset/sequence_length.txt\n"
     ]
    }
   ],
   "source": [
    "# Create NumPy arrays, perform a simple train/test split, and save artifacts\n",
    "\n",
    "X = np.asarray(df[\"padded\"].tolist(), dtype=np.int64)\n",
    "y = df[\"label\"].values.astype(np.int64)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Label vector shape:\", y.shape)\n",
    "print(\"Label distribution (0=Negative, 1=Neutral, 2=Positive):\")\n",
    "(unique, counts) = np.unique(y, return_counts=True)\n",
    "print(dict(zip(unique.astype(int), counts)))\n",
    "\n",
    "# 80/20 train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Save processed arrays and metadata for the RNN model notebook\n",
    "X_train_path = os.path.join(base_dir, \"X_train.npy\")\n",
    "X_test_path = os.path.join(base_dir, \"X_test.npy\")\n",
    "y_train_path = os.path.join(base_dir, \"y_train.npy\")\n",
    "y_test_path = os.path.join(base_dir, \"y_test.npy\")\n",
    "\n",
    "np.save(X_train_path, X_train)\n",
    "np.save(X_test_path, X_test)\n",
    "np.save(y_train_path, y_train)\n",
    "np.save(y_test_path, y_test)\n",
    "\n",
    "# Save vocab and label mapping via pickle\n",
    "vocab_path = os.path.join(base_dir, \"vocab.pkl\")\n",
    "label_map_path = os.path.join(base_dir, \"label_mapping.pkl\")\n",
    "seq_len_path = os.path.join(base_dir, \"sequence_length.txt\")\n",
    "\n",
    "with open(vocab_path, \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open(label_map_path, \"wb\") as f:\n",
    "    pickle.dump(label2idx, f)\n",
    "\n",
    "with open(seq_len_path, \"w\") as f:\n",
    "    f.write(str(sequence_length))\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"  \", X_train_path)\n",
    "print(\"  \", X_test_path)\n",
    "print(\"  \", y_train_path)\n",
    "print(\"  \", y_test_path)\n",
    "print(\"  \", vocab_path)\n",
    "print(\"  \", label_map_path)\n",
    "print(\"  \", seq_len_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aac849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
