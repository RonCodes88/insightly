{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cdd715",
   "metadata": {},
   "source": [
    "# Insightly - The Recurrent Neural Network Implementation\n",
    "## RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452a1e4",
   "metadata": {},
   "source": [
    "### Author: Ronald Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92669070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "base_dir = \"ecommerce_dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6d8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3200, 80)\n",
      "X_test shape: (800, 80)\n",
      "y_train shape: (3200,)\n",
      "y_test shape: (800,)\n",
      "Vocab size: 2944\n",
      "Sequence length: 80\n",
      "Label mapping: {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed NumPy arrays and metadata\n",
    "\n",
    "import pickle\n",
    "\n",
    "X_train = np.load(os.path.join(base_dir, \"X_train.npy\"))\n",
    "X_test = np.load(os.path.join(base_dir, \"X_test.npy\"))\n",
    "y_train = np.load(os.path.join(base_dir, \"y_train.npy\"))\n",
    "y_test = np.load(os.path.join(base_dir, \"y_test.npy\"))\n",
    "\n",
    "with open(os.path.join(base_dir, \"vocab.pkl\"), \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(base_dir, \"label_mapping.pkl\"), \"rb\") as f:\n",
    "    label2idx = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(base_dir, \"sequence_length.txt\"), \"r\") as f:\n",
    "    sequence_length = int(f.read().strip())\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_classes = len(label2idx) \n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Sequence length:\", sequence_length)\n",
    "print(\"Label mapping:\", label2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c62d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Number of training batches: 100\n",
      "Number of test batches: 25\n"
     ]
    }
   ],
   "source": [
    "# Build PyTorch datasets and data loaders\n",
    "\n",
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "X_test_tensor = torch.LongTensor(X_test)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of training batches:\", len(train_loader))\n",
    "print(\"Number of test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9aae4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(2944, 64, padding_idx=0)\n",
      "  (rnn): RNN(64, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Total parameters: 196,931\n"
     ]
    }
   ],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_size: int = 64, num_classes: int = 3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)                 # (batch, seq_len, emb)\n",
    "        out, _ = self.rnn(embedded)                  # (batch, seq_len, hidden)\n",
    "        last_hidden = out[:, -1, :]                  # (batch, hidden)\n",
    "        logits = self.fc(last_hidden)                # (batch, num_classes)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SentimentRNN(vocab_size=vocab_size, num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a951425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting baseline training...\n",
      "\n",
      "Epoch 01/20 | Train Loss: 0.2486 | Train Acc: 0.9463 | Test Loss: 0.2685 | Test Acc: 0.9363\n",
      "Epoch 02/20 | Train Loss: 0.2472 | Train Acc: 0.9463 | Test Loss: 0.2637 | Test Acc: 0.9375\n",
      "Epoch 03/20 | Train Loss: 0.2441 | Train Acc: 0.9459 | Test Loss: 0.2751 | Test Acc: 0.9363\n",
      "Epoch 04/20 | Train Loss: 0.2441 | Train Acc: 0.9466 | Test Loss: 0.2733 | Test Acc: 0.9363\n",
      "Epoch 05/20 | Train Loss: 0.2446 | Train Acc: 0.9459 | Test Loss: 0.2746 | Test Acc: 0.9363\n",
      "Epoch 06/20 | Train Loss: 0.2442 | Train Acc: 0.9466 | Test Loss: 0.2705 | Test Acc: 0.9375\n",
      "Epoch 07/20 | Train Loss: 0.2434 | Train Acc: 0.9466 | Test Loss: 0.2685 | Test Acc: 0.9375\n",
      "Epoch 08/20 | Train Loss: 0.2437 | Train Acc: 0.9466 | Test Loss: 0.2674 | Test Acc: 0.9387\n",
      "Epoch 09/20 | Train Loss: 0.2434 | Train Acc: 0.9463 | Test Loss: 0.2760 | Test Acc: 0.9350\n",
      "Epoch 10/20 | Train Loss: 0.2441 | Train Acc: 0.9466 | Test Loss: 0.2706 | Test Acc: 0.9363\n",
      "Epoch 11/20 | Train Loss: 0.2431 | Train Acc: 0.9466 | Test Loss: 0.2695 | Test Acc: 0.9363\n",
      "Epoch 12/20 | Train Loss: 0.2431 | Train Acc: 0.9466 | Test Loss: 0.2724 | Test Acc: 0.9363\n",
      "Epoch 13/20 | Train Loss: 0.2416 | Train Acc: 0.9466 | Test Loss: 0.2705 | Test Acc: 0.9375\n",
      "Epoch 14/20 | Train Loss: 0.2418 | Train Acc: 0.9466 | Test Loss: 0.2726 | Test Acc: 0.9375\n",
      "Epoch 15/20 | Train Loss: 0.2420 | Train Acc: 0.9466 | Test Loss: 0.2734 | Test Acc: 0.9375\n",
      "Epoch 16/20 | Train Loss: 0.2417 | Train Acc: 0.9466 | Test Loss: 0.2744 | Test Acc: 0.9363\n",
      "Epoch 17/20 | Train Loss: 0.2423 | Train Acc: 0.9466 | Test Loss: 0.2764 | Test Acc: 0.9363\n",
      "Epoch 18/20 | Train Loss: 0.2416 | Train Acc: 0.9466 | Test Loss: 0.2761 | Test Acc: 0.9363\n",
      "Epoch 19/20 | Train Loss: 0.2426 | Train Acc: 0.9466 | Test Loss: 0.2866 | Test Acc: 0.9337\n",
      "Epoch 20/20 | Train Loss: 0.2398 | Train Acc: 0.9466 | Test Loss: 0.2781 | Test Acc: 0.9363\n",
      "\n",
      "Training finished.\n",
      "Saved training history to: ecommerce_dataset/baseline_history.npz\n",
      "Saved model weights to: ecommerce_dataset/baseline_rnn.pt\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting baseline training...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ----- Training -----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # ----- Evaluation -----\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    avg_test_loss = running_loss / total\n",
    "    test_acc = correct / total\n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f} | Test Acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# Save training history and model for the analysis notebook\n",
    "history_path = os.path.join(base_dir, \"baseline_history.npz\")\n",
    "model_path = os.path.join(base_dir, \"baseline_rnn.pt\")\n",
    "\n",
    "np.savez(\n",
    "    history_path,\n",
    "    train_losses=np.array(train_losses),\n",
    "    test_losses=np.array(test_losses),\n",
    "    train_accuracies=np.array(train_accuracies),\n",
    "    test_accuracies=np.array(test_accuracies),\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(\"Saved training history to:\", history_path)\n",
    "print(\"Saved model weights to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b34cb5",
   "metadata": {},
   "source": [
    "### Testing with Custom Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a261f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, remove non-letter characters, normalize whitespace.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"Very simple whitespace tokenization.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokens_to_ids(tokens):\n",
    "    return [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    if len(seq) >= max_len:\n",
    "        return seq[:max_len]\n",
    "    return seq + [pad_value] * (max_len - len(seq))\n",
    "\n",
    "\n",
    "# Map predicted indices back to label names\n",
    "idx2label = {v: k for k, v in label2idx.items()}\n",
    "\n",
    "\n",
    "def predict_sentiments(texts, model_to_use):\n",
    "    \"\"\"Run the trained 3-class sentiment model on a list of raw text reviews.\"\"\"\n",
    "    model_to_use.eval()\n",
    "\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        cleaned = clean_text(text)\n",
    "        tokens = tokenize(cleaned)\n",
    "        ids = tokens_to_ids(tokens)\n",
    "        padded = pad_sequence(ids, sequence_length)\n",
    "        processed.append(padded)\n",
    "\n",
    "    X_new = torch.LongTensor(processed).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_to_use(X_new)  # (batch, num_classes)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred_indices = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        probs_np = probs.cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for text, idx, prob_vec in zip(texts, pred_indices, probs_np):\n",
    "        label = idx2label[int(idx)]\n",
    "        confidence = float(prob_vec[idx])\n",
    "        results.append({\"text\": text, \"label\": label, \"confidence\": confidence})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! Best purchase ever!\",\n",
    "    \"Terrible quality, complete waste of money. Very disappointed.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Good value for the price. Pretty satisfied with this purchase.\",\n",
    "    \"Worst product I've ever bought. Do not recommend!\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104f352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model predictions (unweighted):\n",
      "Text: This product is absolutely amazing! Best purchase ever!\n",
      "Predicted sentiment: Positive (confidence 0.947)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Terrible quality, complete waste of money. Very disappointed.\n",
      "Predicted sentiment: Positive (confidence 0.947)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: It's okay, nothing special but does the job.\n",
      "Predicted sentiment: Positive (confidence 0.947)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Good value for the price. Pretty satisfied with this purchase.\n",
      "Predicted sentiment: Positive (confidence 0.947)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Worst product I've ever bought. Do not recommend!\n",
      "Predicted sentiment: Positive (confidence 0.947)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline model predictions (unweighted):\")\n",
    "for r in predict_sentiments(sample_reviews, model):\n",
    "    print(\"Text:\", r[\"text\"])\n",
    "    print(f\"Predicted sentiment: {r['label']} (confidence {r['confidence']:.3f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe972402",
   "metadata": {},
   "source": [
    "### Model Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e38f6",
   "metadata": {},
   "source": [
    "Recall that about 94% of the reviews are positive, with very few negative or neutral ones, so positive reviews are heavily overrepresented in the data. We address this imbalance by using class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44832ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class counts in training set:\n",
      "  class 0: 74 samples\n",
      "  class 1: 127 samples\n",
      "  class 2: 2999 samples\n",
      "Class weights (used in weighted loss): [14.41441441  8.39895013  0.35567411]\n"
     ]
    }
   ],
   "source": [
    "weighted_model = SentimentRNN(vocab_size=vocab_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Compute class weights from training labels \n",
    "class_counts = np.bincount(y_train)\n",
    "print(\"\\nClass counts in training set:\")\n",
    "for idx, count in enumerate(class_counts):\n",
    "    print(f\"  class {idx}: {count} samples\")\n",
    "\n",
    "class_weights = len(y_train) / (len(class_counts) * class_counts)\n",
    "print(\"Class weights (used in weighted loss):\", class_weights)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion_weighted = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer_weighted = torch.optim.Adam(weighted_model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216e86f",
   "metadata": {},
   "source": [
    "Let's retrain the model using a class-weighted cross-entropy loss to address the strong class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34ceac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with class-weighted loss...\n",
      "\n",
      "[Weighted] Epoch 01/20 | Train Loss: 0.9686 | Train Acc: 0.8119 | Test Loss: 1.0570 | Test Acc: 0.6937\n",
      "[Weighted] Epoch 02/20 | Train Loss: 0.9720 | Train Acc: 0.9072 | Test Loss: 1.0835 | Test Acc: 0.9137\n",
      "[Weighted] Epoch 03/20 | Train Loss: 0.9699 | Train Acc: 0.8016 | Test Loss: 1.0745 | Test Acc: 0.8175\n",
      "[Weighted] Epoch 04/20 | Train Loss: 0.9748 | Train Acc: 0.8341 | Test Loss: 1.0719 | Test Acc: 0.9113\n",
      "[Weighted] Epoch 05/20 | Train Loss: 0.9493 | Train Acc: 0.8109 | Test Loss: 1.0874 | Test Acc: 0.7400\n",
      "[Weighted] Epoch 06/20 | Train Loss: 0.9712 | Train Acc: 0.7966 | Test Loss: 1.1021 | Test Acc: 0.9012\n",
      "[Weighted] Epoch 07/20 | Train Loss: 0.9733 | Train Acc: 0.8231 | Test Loss: 1.0843 | Test Acc: 0.6325\n",
      "[Weighted] Epoch 08/20 | Train Loss: 0.9430 | Train Acc: 0.9009 | Test Loss: 1.0958 | Test Acc: 0.9137\n",
      "[Weighted] Epoch 09/20 | Train Loss: 0.9566 | Train Acc: 0.8078 | Test Loss: 1.1087 | Test Acc: 0.8250\n",
      "[Weighted] Epoch 10/20 | Train Loss: 0.9595 | Train Acc: 0.9066 | Test Loss: 1.0992 | Test Acc: 0.8750\n",
      "[Weighted] Epoch 11/20 | Train Loss: 0.9631 | Train Acc: 0.8922 | Test Loss: 1.0905 | Test Acc: 0.9163\n",
      "[Weighted] Epoch 12/20 | Train Loss: 0.9559 | Train Acc: 0.8950 | Test Loss: 1.1060 | Test Acc: 0.7850\n",
      "[Weighted] Epoch 13/20 | Train Loss: 0.9511 | Train Acc: 0.8741 | Test Loss: 1.1465 | Test Acc: 0.7937\n",
      "[Weighted] Epoch 14/20 | Train Loss: 0.9595 | Train Acc: 0.8828 | Test Loss: 1.0783 | Test Acc: 0.9062\n",
      "[Weighted] Epoch 15/20 | Train Loss: 0.9193 | Train Acc: 0.7747 | Test Loss: 1.1305 | Test Acc: 0.7588\n",
      "[Weighted] Epoch 16/20 | Train Loss: 0.9407 | Train Acc: 0.9163 | Test Loss: 1.1060 | Test Acc: 0.9100\n",
      "[Weighted] Epoch 17/20 | Train Loss: 0.9305 | Train Acc: 0.8284 | Test Loss: 1.1062 | Test Acc: 0.8275\n",
      "[Weighted] Epoch 18/20 | Train Loss: 0.9406 | Train Acc: 0.9153 | Test Loss: 1.1196 | Test Acc: 0.7700\n",
      "[Weighted] Epoch 19/20 | Train Loss: 0.9738 | Train Acc: 0.7531 | Test Loss: 1.0873 | Test Acc: 0.7788\n",
      "[Weighted] Epoch 20/20 | Train Loss: 0.9863 | Train Acc: 0.8556 | Test Loss: 1.1024 | Test Acc: 0.7725\n",
      "\n",
      "Weighted training finished.\n",
      "Saved weighted training history to: ecommerce_dataset/baseline_weighted_history.npz\n",
      "Saved weighted model weights to: ecommerce_dataset/baseline_rnn_weighted.pt\n"
     ]
    }
   ],
   "source": [
    "EPOCHS_WEIGHTED = 20\n",
    "\n",
    "train_losses_w = []\n",
    "train_accuracies_w = []\n",
    "test_losses_w = []\n",
    "test_accuracies_w = []\n",
    "\n",
    "print(\"\\nStarting training with class-weighted loss...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS_WEIGHTED + 1):\n",
    "    # ----- Training -----\n",
    "    weighted_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = weighted_model(xb)\n",
    "        loss = criterion_weighted(logits, yb)\n",
    "\n",
    "        optimizer_weighted.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_weighted.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses_w.append(avg_train_loss)\n",
    "    train_accuracies_w.append(train_acc)\n",
    "\n",
    "    # ----- Evaluation -----\n",
    "    weighted_model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = weighted_model(xb)\n",
    "            loss = criterion_weighted(logits, yb)\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    avg_test_loss = running_loss / total\n",
    "    test_acc = correct / total\n",
    "    test_losses_w.append(avg_test_loss)\n",
    "    test_accuracies_w.append(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"[Weighted] Epoch {epoch:02d}/{EPOCHS_WEIGHTED} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f} | Test Acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nWeighted training finished.\")\n",
    "\n",
    "# Save history and model separately so we keep the original artifacts\n",
    "history_path_w = os.path.join(base_dir, \"baseline_weighted_history.npz\")\n",
    "model_path_w = os.path.join(base_dir, \"baseline_rnn_weighted.pt\")\n",
    "\n",
    "np.savez(\n",
    "    history_path_w,\n",
    "    train_losses=np.array(train_losses_w),\n",
    "    test_losses=np.array(test_losses_w),\n",
    "    train_accuracies=np.array(train_accuracies_w),\n",
    "    test_accuracies=np.array(test_accuracies_w),\n",
    ")\n",
    "\n",
    "torch.save(weighted_model.state_dict(), model_path_w)\n",
    "\n",
    "print(\"Saved weighted training history to:\", history_path_w)\n",
    "print(\"Saved weighted model weights to:\", model_path_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4589ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighted model predictions (class-weighted):\n",
      "Text: This product is absolutely amazing! Best purchase ever!\n",
      "Predicted sentiment: Positive (confidence 0.374)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Terrible quality, complete waste of money. Very disappointed.\n",
      "Predicted sentiment: Positive (confidence 0.389)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: It's okay, nothing special but does the job.\n",
      "Predicted sentiment: Neutral (confidence 0.423)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Good value for the price. Pretty satisfied with this purchase.\n",
      "Predicted sentiment: Positive (confidence 0.431)\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Worst product I've ever bought. Do not recommend!\n",
      "Predicted sentiment: Positive (confidence 0.494)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeighted model predictions (class-weighted):\")\n",
    "for r in predict_sentiments(sample_reviews, weighted_model):\n",
    "    print(\"Text:\", r[\"text\"])\n",
    "    print(f\"Predicted sentiment: {r['label']} (confidence {r['confidence']:.3f})\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eecb1d",
   "metadata": {},
   "source": [
    "While the weighted model did not improve overall performance, let's check the per-class metrics like precision/recall for Positive/Neutral/Negative using a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39340bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model (unweighted)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.25      0.11      0.15        19\n",
      "     Neutral       1.00      0.10      0.18        31\n",
      "    Positive       0.94      0.99      0.97       750\n",
      "\n",
      "    accuracy                           0.94       800\n",
      "   macro avg       0.73      0.40      0.43       800\n",
      "weighted avg       0.93      0.94      0.92       800\n",
      "\n",
      "\n",
      "Weighted model (class-weighted loss)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.07      0.05      0.06        19\n",
      "     Neutral       0.04      0.16      0.06        31\n",
      "    Positive       0.94      0.82      0.87       750\n",
      "\n",
      "    accuracy                           0.77       800\n",
      "   macro avg       0.35      0.34      0.33       800\n",
      "weighted avg       0.88      0.77      0.82       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_on_test(model_to_use, name: str):\n",
    "    \"\"\"Print a classification report for the given model on the test set.\"\"\"\n",
    "    model_to_use.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model_to_use(xb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    # Ensure labels are in the correct order 0, 1, 2 -> Negative, Neutral, Positive\n",
    "    label_order = sorted(idx2label.keys())\n",
    "    target_names = [idx2label[i] for i in label_order]\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(classification_report(all_labels, all_preds, labels=label_order, target_names=target_names))\n",
    "\n",
    "\n",
    "# Compare baseline and class-weighted models on the test set\n",
    "evaluate_model_on_test(model, \"Baseline model (unweighted)\")\n",
    "evaluate_model_on_test(weighted_model, \"Weighted model (class-weighted loss)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27c07f",
   "metadata": {},
   "source": [
    "It looks like the classâ€‘weighted model is worse overall and only slightly better at catching minority examples, with many more false positives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
